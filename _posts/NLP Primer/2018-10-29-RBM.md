---
layout : blog
title: 神经网络与RBM
category : NLP-Primer
duoshuo: true
date : 2018-10-29
---

# 神经网络的前世今生
## 前言
网络上对波尔兹曼机讨论甚少，所以本文是对RBM的一个简介。

Geoffrey Hinton 被称为`"深度学习之父"`与`"神经网络先驱"`，他的主要贡献包括反向传播算法、受限波尔兹曼机、深度置信网络、对比散度算法、ReLU激活单元、Dropout方法，在2017年末又提出胶囊神经网络，改善了CNN。

## 反向传播
`Back Propagation`，很难相信它在生物学上成立，并且他需要使用SGD等方法进行优化，这是一个高度非凸的问题，相比之下，SVM显然可解释性更高也更优雅。
* 收敛速度慢，使用Newton法解决
* 过拟合，Dropout方法解决
* 局部极小，使用不同的初始值

## 限制波尔兹曼机RBM
在第二次NN低谷后，Hinton寻找到了另一个模型：`统计热力学模型`，根据波尔兹曼统计相关知识，结合马尔科夫随机场等图学习理论，提出波尔兹曼机(BM)。使用能量模型描述神经网络，更加可解释。

### 人工神经网络
* 前馈结构(Feed Forward)，没有循环，静态的网络
* 反馈结构(Feedback/ Recurrent)，有循环，是动态的网络

### HopField网络
它是反馈结构的神经网络，它具有内容寻址记忆的特点，通过与数据部分相似的输入，回想起数据本身。

### 波尔兹曼机
也是反馈结构，但是他服从波尔兹曼分布，具有隐单元。

### RBM
他是波尔兹曼机的限制版本，限制可见层(v，输入)与隐层(h，输出)之间有连接，但是层内无连接，这就不构成循环结构。值得注意的是，上述三个网络的神经元只有**0/1**状态，即激活与未激活状态。

在统计热力学中，波尔兹曼分布（Gibson分布），用于描述量子体系量子态分布： $P(s) \propto e^{\frac{-E(s)}{kT}}$，其中$s$为量子态，$E(s)$为状态的能量,$P(s)$为这个状态出现的概率，$k$为波尔兹曼常熟，$T$是系统温度(常数)，所以可以假设$kT=1$，可以简化为
$$P(s) \propto e^{-E(s)}$$
可以得到每个量子的概率为
$$P(s_{i}) = \frac{ e^{-E(s_{i})} }{ \sum_{s}e^{-E(s)} }$$
这和**softmax**相同，我们可以再次定义$Z := \sum_{s}e^{-E(s)}$，于是有$P(s) = \frac{1}{Z} e^{-E(s)}$。

为了把这个模型应用在神经网络中，我们需要定义$E$与$s$在神经网络中的角色。前面说过，经典的人工神经网络中，具有**可见层**与**隐层**(中间层)，所以定义$s$为可见层并上隐层的状态，即$s=(v, h)$，$P(v,h) = \frac{1}{Z}e^{-E(v, h)}$。同样巧合的是，物理学中的**易辛(Ising)模型**，与神经网络极其相似，对于神经元的偏置作为Ising Model的外场，权重$W$作为内部耦合系数(两个神经元之间的权重越大，代表耦合程度越高，关联越强)，得到如下能量公式，
$$E(v, h) = -a^{T}v-b^{T}h-h^{T}Wv$$
可以看出，对于可见层的偏置设为$a$，而对于隐藏层的偏置则为$b$。如果我们将某个神经元$h_{i}$的能量分离出来，也就是
$$E(v, h) = -a^{T}v - b'^{T}vh'-h'^{T}W'v - h_{i}(W_{i}v+b_{i})$$
将前半段式子设为$E(v,h')=-a^{T}v - b'^{T}vh'-h'^{T}W'v$，那么我们就可以得到$P(v, h) = \frac{1}{Z} e^{-E(v, h')} e^{h_{i}W_{i}v+b_{i}}$，很容易得到
$$\begin{array}{l}
\quad P(h_{i}=1|v) &=& \frac{ \sum_{h',h_{i}=1}P(v,h) } { \sum_{h',h_{i}=0}P(v,h) + \sum_{h',h_{i}=1}P(v,h) } \\
&=& \frac{1}{1 + \frac{ \sum_{h',h_{i}=0}P(v,h) }{ \sum_{h',h_{i}=1}P(v,h) } } \\
&=& \frac{1} {1 + \frac{\sum_{h'}E(v,h')}{ \sum_{h'}E(v,h')e^{W_{i}v+b_{i}} } } \\
&=& \frac{1}{1+e^{-(w_{i}v+b_{i})}} \\
\end{array}$$
再次得到了Sigmoid函数，也就是
$$P(h_{i}=1|v) = \sigma(W_{i}v+b)$$
这时候，Sigmoid函数的解释为**波尔兹曼分布下隐含层神经元激活的条件概率的激活函数**。

优化的目标，就是极大似然估计，也就是最大化$P(v)=\frac{1}{Z} \sum_{h}e^{-E(v, h)}$，这个方程与热力统计学中的自由能十分相似$F(v)=-ln\sum_{h}e^{-E(v, h)}$，而式中$Z=\sum_{v}e^{-F(v)}$，于是有$P(v)=\frac{1}{Z}e^{-F(v)}$，而我们的目标是使能量最低的一组参数，在论文`A practical guide to training restricted Boltzmann machines Momentum`中可以看到更多细节。

不过优化整体网络是很困难的，其根源性在于分配函数$Z$，通常认为这是一个**P-Hard**问题，如果能够解决，那么很多热力学系统，包括Ising模型也就迎刃而解了。

算法中很重要的手段是**贪心**，逐层训练网络，而不是整体优化，为了训练每层RBM，Hinton提出了对比散度(Contrastive divergence)算法。利用Gibbs采样，但是收敛速度仍然很慢，所以再次近似固定采样步数$k=1$，**这时候效果已经相当好了**。这个算法是**无监督**的学习，不需要标签就可以调优。

不过，后来由于使用ReLU以及合适的初始化，加上CNN，搭配GPU，原来的深度神经网络可以照常训练，不需要使用RBM预训练。在监督学习方面，效果不如直接反向传播，所以最近很少有人再提起，所以如今神经网络模型与30年前(LSTM、CNN)没什么差别，只有trick上的差距。

物理定律是一部分不能用数学证明的真理。

### 深度置信网络(Deep Belief Networks)
由多个RBM组成，每次训练一个RBM。

## Todo List
- [ ] 插入图片